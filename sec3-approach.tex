%%% squeeze equations
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\setlength{\abovedisplayshortskip}{2pt}
\setlength{\belowdisplayshortskip}{2pt} 
\allowdisplaybreaks

\section{Retrieval Framework}
\label{sec:approach}
In the standard ad hoc retrieval setting, a query $\query$ is used to
represent an information need $\infoNeed$ for retrieval over a
document corpus $\corpus$.
Modeling the information need for retrieval is a challenging task.
Indeed, there has been a large body of work on devising
representations using various retrieval frameworks and paradigms.

The retrieval setting we address here is different.
Rather than having a single query $\query$ representing the
information need $\infoNeed$, we assume a set $\querySet$ of
$\numQuery$ queries, $\query_1, \ldots,\query_{\numQuery}$, each of
which represents $\infoNeed$.
Accordingly, the challenge becomes devising an information need
representation using the queries in $\querySet$ --- our main focus in
what follows.

To address the information-need representation challenge, we appeal
to the generative framework for relevance~\cite{Lavrenko+Croft:03a}.
The framework is formally grounded and constitutes the basis for a
highly effective retrieval paradigm, namely, the relevance
model~\cite{Lavrenko+Croft:03a,Abdul-Jaleel+al:04a}.

\subsection{The Generative Theory for Relevance}
\label{sec:genTheory}

The fundamental generative assumption for relevance is
\cite{Lavrenko+Croft:03a}:
\begin{assumption}
\label{assum:origGen}
{\em Given a query $\query$, there exists a relevance language model,
$\relModel$, that generates terms in $\query$ and in documents
relevant to $\query$.
}
\end{assumption}
Once estimated, the relevance model $\relModel$ which serves as a representation
of $\infoNeed$ is used to rank documents by its similarity to their induced language models~\cite{Lavrenko+Croft:03a}.

The fact that relevance is determined with
respect to the (unknown) information need $\infoNeed$ and not with
respect to the query $\query$ and that an information need can be
represented using various queries, gives rise to a natural extension
of the generative assumption for relevance to the case of multiple
queries:
\begin{assumption}
\label{assum:extend}
  Given a set of queries $\querySet$, each 
  representing the information need $\infoNeed$, there exists a relevance
  language model, $\relModel$, that generates the terms in these
  queries and in documents relevant to $\infoNeed$.
\end{assumption}

Given this assumption, our task becomes estimating a relevance model,
$\relModel$, using a set of queries $\querySet$.
To that end, we first describe notational conventions that will be
used throughout this section.
In Section \ref{sec:singleQuery} we describe the standard approach to
estimating a relevance model using a single query.
Then, in Section \ref{sec:multQueries}, we describe a suite of
approaches for inducing a relevance model from the set of queries
$\querySet$.

\myparagraph{Notational conventions}
We use unigram language models.
The maximum likelihood estimate (MLE) of term $\arbTerm$ with respect
to the text (or text collection) $\arbText$ is
$\mleProb{\arbTerm}{\arbText} \definedas
\frac{\freq{\arbTerm}{\arbText}}{\vert \arbText \vert}$, where
$\freq{\arbTerm}{\arbText}$ is the number of occurrences of
$\arbTerm$ in $\arbText$ and $\vert \arbText \vert$ is the number of
term occurrences in $\arbText$.
The probability assigned to $\arbTerm$ by a Dirichlet smoothed
language model induced from $\arbText$ is
\cite{Zhai+Lafferty:01a}: $ \dirProb{\arbTerm}{\arbText} \definedas
\frac{\freq{\arbTerm}{\arbText} + \dirParam}{\vert \arbText \vert +
\dirParam \mleProb{\arbTerm}{\corpus}}$; $\dirParam$ is the smoothing
parameter.
We compare language models $\arbLMone$ and $\arbLMtwo$ using 
cross entropy: $\ceSmall{\arbLMone}{\arbLMtwo} \definedas -
\sum_{\arbTerm}\condP{\arbTerm}{\arbLMone} ~{\log}~
\condP{\arbTerm}{\arbLMtwo}$; higher values correspond to decreased
similarity.
Specifically, we rank documents in the corpus with respect to any
query model (relevance models and MLEs) by the minus cross entropy
between the query model and the document Dirichlet smoothed language
model.


\subsection{Single Query Relevance Models}
\label{sec:singleQuery}
The standard approach to estimating a relevance model $\relModel$
using a query $\query$ is based on the approximation
\cite{Lavrenko+Croft:03a}:
\begin{equation}
\label{eq:basicApprox}
\condP{\arbTerm}{\relModel} \approx \condP{\arbTerm}{\query}.
\end{equation}
The probability of generating $\arbTerm$ from
$\relModel$ is approximated by the probability of ``observing''
$\arbTerm$ given that $\query$'s terms have been ``observed''.

Relevance model \#1, {\rmOne}, is estimated using a pseudo feedback
approach.
Specifically, let $\retListParm{\ql}{\query}$ be the list of
documents most highly ranked by the query likelihood method (\ql)
\cite{Song+Croft:99a} that scores document $\doc$ by $\prod_{\arbTerm
\in \query}
\dirProb{\arbTerm}{\doc}$.
Then, \rmOne is a linear mixture of language models induced from the documents in $\retListParm{\ql}{\query}$:
\begin{equation}
\label{eq:rm1}
\condP{\arbTerm}{\rmOne} \approx \condP{\arbTerm}{\query} \definedas \sum_{\doc\in \retListParm{\ql}{\query}} \dirProb{\arbTerm}{\doc}\condP{\doc}{\query};
\end{equation} 
\begin{equation}
\label{eq:normQL}
\condP{\doc}{\query}\definedas\frac{\dirProb{\query}{\doc}}{\sum_{\doc' \in \retListParm{\ql}{\query}}\dirProb{\query}{\doc'}}
\end{equation}
is $\doc$'s normalized query likelihood.
We note that using documents in $\retListParm{\ql}{\query}$ in
Equation \ref{eq:rm1} is a practical approximation for using all
documents in the corpus.
Indeed, $\condP{\doc}{\query}$ is the highest for documents in
$\retListParm{\ql}{\query}$ by virtue of the way
$\retListParm{\ql}{\query}$ was created; and, $\condP{\doc}{\query}$
significantly drops for documents ranked low by the initial query
likelihood retrieval~{\cite{Lavrenko+Croft:03a}}.
We re-visit this point below.

It is standard practice to clip pseudo-feedback-based query 
models, by setting to zero the
probabilities of all but the $\numClip$ terms assigned the highest
probability by the
model~\cite{Zhai+Lafferty:01a,Abdul-Jaleel+al:04a}; re-normalization
is applied to yield a valid probability distribution denoted
$\condP{\cdot}{\modelClip{\rmOne}}$.

Finally, $\modelClip{\rmOne}$ is anchored to the original query
$\query$ to ameliorate potential query
drift~\cite{Abdul-Jaleel+al:04a}.
The result is relevance model \#3 (\rmThree):
\begin{equation}
\label{eq:rm3}
\condP{\arbTerm}{\rmThree} \definedas (1-\lambda) \mleProb{\arbTerm}{\query} + \lambda \condP{\arbTerm}{\modelClip{\rmOne}};
\end{equation} 
$\lambda$ is a free parameter.
In what follows, we use the notation $\relModelParm{\query}$ to refer
to {\rmThree} induced using Equation \ref{eq:rm3}.


\subsection{Multi-Query Relevance Models}
\label{sec:multQueries}
We now address the novel challenge that emerges from the retrieval
setting we address here; that is, representing the information need
using relevance modeling and multiple queries.

Given the set of queries $\querySet$, we can use an approximation
analogous to that in Equation \ref{eq:basicApprox} to estimate a
relevance model:
\begin{equation}
\label{eq:secondApprox}
\condP{\arbTerm}{\relModel} \approx \condP{\arbTerm}{\querySet}.
\end{equation}
In other words, the probability to generate $\arbTerm$ from the
relevance model is approximated by the probability to observe
$\arbTerm$ given that the queries in $\querySet$ have been observed.

Let $\set{\queryRV_i}_{i=1}^{\numQuery}$ be a set of random
variables, each takes queries as values.
Assuming that these random variables are {\em exchangeable} (order
invariant), we get by de Finetti's representation theorem
\cite{finetti:90a} that: $$p(\queryRV_1 =
\query_1,\ldots,\queryRV_{\numQuery}=\query_{\numQuery} ) =
\int_{\relModel} \big (\prod_{i=1}^{\numQuery}  \condP{\query_i}{\relModel} \big)
\prob(\relModel) d \relModel.$$ 
The implication is that we can assume that the queries in $\querySet$
are {\em conditionally} independent given $\relModel$.
%\footnote{But
%this does not mean that the queries are (absolutely) independent.}
We next turn to describing methods of estimating
$\condP{\arbTerm}{\querySet}$ so as to induce $\relModel$ using
Equation \ref{eq:secondApprox}.

\subsubsection{Fusing Queries}
\label{sec:fuseQueries}
\label{sec:reduceSingle}
A simple approach to estimating $\condP{\arbTerm}{\querySet}$ is
representing $\querySet$ as a single query --- e.g., fusing the terms
of queries in $\querySet$ --- and using the relevance-model estimates
from Section~{\ref{sec:singleQuery}}.
Here, we concatenate the queries ($\concat$ is the concatenation operator):
$$\concatQuery \definedas
\concat_{\query_i \in \querySet} \query_i;$$
concatenation order has no effect given the exchangeability
assumption stated above.
The resultant relevance model, \firstmention{\conRM}, is:
\begin{equation}
\label{eq:concatRM}
\condP{\arbTerm}{\relModel_{\svar{\conRM}}} \approx \condP{\arbTerm}{\querySet} \definedas \condP{\arbTerm}{\relModelParm{\concatQuery}}.
\end{equation}

As a reference comparison, we use \firstmention{\conMLE}: an
unsmoothed maximum likelihood estimate,
$\mleProb{\cdot}{\concatQuery}$, is clipped to use $\numClip$
terms\footnote{Term clipping is applied to all query models used for
retrieval --- see Section \ref{sec-results}.}
and then utilized directly for retrieval; pseudo-feedback-based
relevance modeling is not used.
Note that long queries affect $\concatQuery$ to a larger extent than
short queries.
The estimates we describe below address this shortcoming.

\subsubsection{Fusing Relevance Models}
\label{sec:fuseRelModels}
The random variables, $\set{\queryRV_i}_{i=1}^{\numQuery}$, take
queries of the same type (keyword queries) as values; the variables
were assumed to be exchangeable.
Hence, we can use a single random variable, $\queryRV$, that takes
the queries in $\querySet$ as values:
\begin{equation}
\label{eq:partition}
\estcondP{\arbTerm}{\querySet} \definedas \sum_{\query_i \in \querySet} \estcondP{\arbTerm}{\queryRV = \query_i}\estcondP{\queryRV = \query_i}{\querySet}.
\end{equation}
Herein, $\estprob$ denotes an estimate for $\prob$.
Equation \ref{eq:partition} is based on the assumption that given $\queryRV$, $\arbTerm$ is independent of $\querySet$; and, that
$\estcondP{\queryRV}{\querySet}$ is a valid probability distribution:
$\sum_{\query_i \in \querySet} \estcondP{\queryRV =
\query_i}{\querySet} = 1$.

We assume that queries are drawn from $\querySet$ using a uniform
distribution: $\estcondP{\queryRV =
\query_i}{\querySet} \definedas
\frac{1}{\numQuery}$.\footnote{Alternatively, we could estimate
$\condP{\queryRV = \query_i}{\querySet}$ by a measure of $\query_i$'s
representativeness of $\querySet$; e.g., its similarity to other
queries in $\querySet$.
We leave this for future work.}
Now, using a relevance-model estimate based on Equation \ref{eq:basicApprox} for
$\estcondP{\arbTerm}{\queryRV = \query_i}$ yields the
\firstmention{\ariRM} estimate which linearly fuses the relevance models
$\relModelParm{\query_i}$:
\begin{equation}
\label{eq:arith}
\condP{\arbTerm}{\relModel_{\svar{\ariRM}}} \approx \estcondP{\arbTerm}{\querySet}   \definedas \frac{1}{\numQuery} \sum_{\query_i \in \querySet} \condP{\arbTerm} {\relModelParm{\query_i}}.
\end{equation}
%In other words, we fuse the relevance models
%$\relModelParm{\query_i}$ by using their arithmetic mean.

Alternatively, using $\mleProb{\arbTerm}{\query_i}$ as an
estimate for $\condP{\arbTerm}{\queryRV=\query_i}$ yields
the \firstmention{\ariMLE} estimate which does not rely on pseudo
feedback and relevance modeling:
\begin{equation}
\label{eq:arithMLE}
\prob^{\svar{\ariMLE}}(\arbTerm|\querySet) \definedas \frac{1}{\numQuery} \sum_{\query_i \in \querySet} \mleProb{\arbTerm} {\query_i}.
\end{equation}
If we set $\lambda=0$ in Equation \ref{eq:rm3}, then \method{\ariRM} becomes
\method{\ariMLE}.
In contrast to \method{\conMLE} (see Section
\ref{sec:reduceSingle}), \method{\ariMLE} has no query-length
bias. 

There is an interesting connection between using \method{\ariMLE} for
retrieval and the \method{CombSUM} method for fusing retrieved document lists
\cite{fox1994combination}. \method{CombSUM} assigns document $\doc$ the score:
\begin{equation}
  \label{eq:combsum}
  \mbox{\method{CombSUM}} (\doc) \definedas \sum_{L_i: d \in L_i}\var{Score}(\doc;L_i);
\end{equation}
 $\set{L_i}$ are the document lists to be fused and $\var{Score}(\doc;L_i)$ is $\doc$'s score in $L_i$.
Now, $\doc$'s retrieval score with respect to
$\prob^{\svar{\ariMLE}}(\arbTerm|\querySet)$ is:
\begin{align}
%& -\ceSmall{\prob^{\svar{\ariMLE}}(\cdot|\querySet)}{\dirProb{\cdot}{\doc}} =  \qquad \qquad \qquad \qquad \qquad \quad \notag \\
& -\ceSmall{\prob^{\svar{\ariMLE}}(\cdot|\querySet)}{\dirProb{\cdot}{\doc}} = \qquad \qquad \qquad \qquad \qquad \quad \\
& = \frac{1}{m}  \sum_{\arbTerm} \sum_{\query_i \in \querySet} \mleProb{\arbTerm} {\query_i}\log \dirProb{\arbTerm}{\doc} \notag \\ 
%& \qquad \qquad \frac{1}{m}  \sum_{\arbTerm} \sum_{\query_i \in \querySet} \mleProb{\arbTerm} {\query_i}\log \dirProb{\arbTerm}{\doc} = \notag \\
%& = \frac{1}{m}  \sum_{\arbTerm} \sum_{\query_i \in \querySet} \mleProb{\arbTerm} {\query_i}\log \dirProb{\arbTerm}{\doc}  \notag \\
& = \frac{1}{m} \sum_{\query_i \in \querySet} \sum_{\arbTerm} \mleProb{\arbTerm} {\query_i}\log \dirProb{\arbTerm}{\doc} \notag \\
%& \qquad \qquad \frac{1}{m} \sum_{\query_i \in \querySet} \sum_{\arbTerm} \mleProb{\arbTerm} {\query_i}\log \dirProb{\arbTerm}{\doc}= \notag \\
%& \qquad \qquad - \frac{1}{m}  \sum_{\query_i \in \querySet} \ceSmall{\mleProb{\cdot}{\query_i}}{\dirProb{\cdot}{\doc}}. \notag
& = - \frac{1}{m}  \sum_{\query_i \in \querySet} \ceSmall{\mleProb{\cdot}{\query_i}}{\dirProb{\cdot}{\doc}}. \notag
\end{align}
That is, $\doc$'s retrieval score is rank equivalent to the sum of
its cross-entropy-based scores with respect to the queries in
$\querySet$.
This is essentially {\combsum} fusion of $\doc's$ retrieval scores
with respect to the queries.
There are, however, a few technical differences that set apart
\method{\ariMLE}, when used for retrieval, and {\combsum}.
First, {\combsum} is usually applied over truncated document lists;
in our case, the top documents in a list retrieved for query
$\query_i$.
Furthermore, score-normalization is applied to each list.
In contrast, the implication of the equivalences above is that a
document $\doc$ that contains at least one term from a query in
$\querySet$ will be assigned a non-zero score; and, scores are not
normalized.
Second, if one clips $\prob^{\svar{\ariMLE}}(\cdot|\querySet)$ to
%%-OKc: added ``as we do here''
use only the terms assigned the highest probabilities, as we do in
our experiments, then the equivalences do not hold anymore.


Other aggregation approaches could easily be applied to the
arithmetic-mean based fusion of relevance models used in \method{\ariRM}, or
to the MLE-based models.
Here we consider two additional methods that were applied in tasks
and settings different than ours.

\myparagraph{Geometric mean} Using the geometric mean of document
language models was shown to be an effective approach to represent a
cluster of similar documents~\cite{Liu+Croft:08a} and to
%A geometric mean of document language models was also used to
construct a geometric relevance model from the documents
most highly ranked by the query likelihood model
($\retListParm{\ql}{\query}$)~\cite{Seo+Croft:10a}.
The use of the geometric mean was justified using arguments from the
field of information geometry~\cite{Seo+Croft:10a}.
Here, we use the geometric mean of the relevance models induced using
the queries in $\querySet$ to devise the \firstmention{\geoRM}
relevance-model-based estimate:
\begin{equation}
\label{eq:geo}
%\condP{\arbTerm}{\querySet} \approx
%%-OKc: added paranthesis
\condP{\arbTerm}{\relModel_{\svar{\geoRM}}} \definedas \sqrt[\numQuery]{ \prod_{\query_i \in \querySet} (\condP{\arbTerm} {\relModelParm{\query_i} }+ \epsilon}); 
\end{equation}
$\epsilon = 10^{-6}$ is a smoothing factor.
Similarly, \firstmention{\geoMLE} is the geometric mean of the MLEs
induced from the queries:
\begin{equation}
\label{eq:geoMLE}
\prob^{\svar{\geoMLE}}(\arbTerm|\querySet) \definedas \sqrt[\numQuery]{ \prod_{\query_i \in \querySet} (\mleProb{\arbTerm} {\query_i} + \epsilon)}.
\end{equation}

We note that \method{\geoRM} and \method{\geoMLE} are not valid language models
(without further normalization) as the probabilities over terms do
not sum to 1.
Yet, they can be used to rank documents in the corpus with the cross
entropy measure~\cite{Liu+Croft:08a,Seo+Croft:10a}.

The geometric mean used in \method{\geoRM} and \method{\geoMLE} is more conservative
than the arithmetic mean used in \method{\ariRM} and \method{\ariMLE}: a term assigned
a low probability by one of the fused language models affects the
geometric mean
%%-OKc:
%(much)
more than it affects the arithmetic mean.


\omt{
\myparagraph{Divergence minimization}
Zhai and Lafferty \cite{Zhai+Lafferty:01b} used a divergence
minimization approach to induce a pseudo-feedback-based query model
from top-retrieved documents; specifically, those in
$\retListParm{\ql}{\query}$.\footnote{Originally
\cite{Zhai+Lafferty:01b}, the minus cross entropy between query and
document language models served to induce the initial ranking.
This ranking is rank equivalent to the query likelihood ranking if an
unsmoothed MLE is used as the query model~\cite{Lafferty+Zhai:01a}.}
The idea was to find a language model whose divergence from those
induced from documents in $\retListParm{\ql}{\query}$ is minimal; the
divergence was regularized with the divergence from the corpus
language model.
Later, \citet{Lv+Zhai:14a} improved this (language) model
aggregation technique by using entropy regularization.
We use this approach to devise a relevance-model estimate,
\firstmention{\divRM}, that yields the lowest corpus and entropy
regularized divergence with respect to $\relModelParm{\query_1},
\ldots, \relModelParm{\query_\numQuery}$:
\begin{align}
\label{eq:divMin}
\relModel_{\divRM} & \definedas  \arg\,min_{\arbLM}  \\
& \sum_{\query_i \in \querySet} \ceSmall{\arbLM}{\relModelParm{\query_i}} -\lambda \ceSmall{\arbLM}{\mleProb{\cdot}{\corpus}} - \beta \entropy(\arbLM); \notag
\end{align}
$\theta$ is a unigram language model; $\entropy$ is the entropy
function; $\lambda$ and $\beta$ are free parameters.
This minimization problem has a closed form
solution~\cite{Lv+Zhai:14a}.

As a reference comparison, we use the language model
\firstmention{\divMLE} that minimizes the regularized divergence with
respect to the MLEs: $\mleProb{\cdot}{\query_1}, \ldots,
\mleProb{\cdot}{\query_{\numQuery}}$.
\divMLE does not rely on relevance modeling and pseudo feedback:
\begin{align}
\label{eq:divMinMLE}
&\prob^{\divMLE}(\arbTerm|\querySet)  \definedas \arg\,min_{\arbLM}  \\
& \sum_{\query_i \in \querySet} \ceSmall{\arbLM}{\mleProb{\cdot}{\query_i}} -\lambda \ceSmall{\arbLM}{\mleProb{\cdot}{\corpus}} - \beta \entropy(\arbLM). \notag
\end{align}

}
\myparagraph{Fitting a Dirichlet distribution}
All the (unigram) language models we use are defined over the $\vert
\voc \vert -1$ simplex, where $\voc$ is the vocabulary used in the
corpus.
Thus, the language models can be viewed as points sampled from an
underlying Dirichlet distribution.

Inspired by work on fitting a Dirichlet distribution using
pseudo-feedback-based language models induced from alternations of a
query and/or by sampling pseudo relevant documents \cite{Collins-Thompson+Callan:07a}, we fit a
Dirichlet distribution to the relevance models
$\relModelParm{\query_1},\ldots,\relModelParm{\query_{\numQuery}}$.
A maximum likelihood approach is used for 
fitting~\cite{Minka:00a}.
The mean and mode of the fitted Dirichlet distribution serve as the
\firstmention{\dirRMmean} and \firstmention{\dirRMmode}
relevance-model estimates, respectively.
Similarly, \firstmention{\dirMLEmean} and \firstmention{\dirMLEmode}
are the mean and mode of a Dirichlet distribution fitted directly to
the MLEs: $\set{\mleProb{\cdot}{\query_i}}_{i=1}^{\numQuery}$.

%\input{fig-docfuserm.tex}
\subsection{Fusing Retrieved Results}
\label{sec:fuseRetList}
Each of the relevance models $\relModelParm{\query_{i}}$ is induced
from $\retListParm{\ql}{\query_i}$: the documents most highly ranked
by the query likelihood method with respect to $\query_i$.
Obviously, some of the queries in $\querySet$ are more effective
representations than others for retrieval over the corpus $\corpus$.
Hence, the lists $\retListParm{\ql}{\query_i}$ are of varying
effectiveness.
To leverage the lists so as to improve document-relevance estimates
for the task of relevance model construction, one can apply a fusion
approach over the lists.
Then, a relevance model can be induced from the fused list.
We now turn to formally derive the foundations of this approach.

We use Equation~\ref{eq:secondApprox} to estimate a relevance model.
Let $\docRV$ be a random variable that takes as values documents in
the corpus.
We can estimate $\condP{\arbTerm}{\querySet}$, and therefore
$\condP{\arbTerm}{\relModel}$, as follows:
\begin{equation}
\label{eq:partitionDoc}
\condP{\arbTerm}{\relModel} \approx \estcondP{\arbTerm}{\querySet} \definedas \sum_{\doc_i \in \corpus} \estcondP{\arbTerm}{\docRV = \doc_i}\estcondP{\docRV = \doc_i}{\querySet}.
\end{equation}
The estimate is based on the assumptions that $\arbTerm$ is
independent of $\querySet$ given $\docRV$ and that
$\estcondP{\docRV}{\querySet}$ is a valid probability distribution
over the corpus: $\sum_{\doc_i \in
\corpus}\estcondP{\docRV=\doc_i}{\querySet} =1$.
We factor the estimate $\estcondP{\docRV=\doc_i}{\querySet}$:
$$\estcondP{\docRV = \doc_i}{\querySet} \definedas \sum_{\query_j \in
\querySet}
\estcondP{\docRV=\doc_i}{\queryRV=\query_j}\estcondP{\queryRV=\query_j}{\querySet}.$$
The assumption is that a document is independent of $\querySet$ given
$\queryRV$.
Then, using a uniform distribution for
$\estcondP{\queryRV=\query_j}{\querySet}$ results in: 

\begin{equation}
\label{eq:doubleIndex}
\condP{\arbTerm}{\relModel} \definedas
\sum_{\doc_i \in \corpus} \estcondP{\arbTerm}{\docRV=\doc} \frac{1}{\numQuery} \sum_{\query_j \in \querySet}  \estcondP{\docRV=\doc}{\queryRV=\query_j}.
\end{equation}

To alleviate the computational cost of using all documents in the
corpus to estimate Equation~\ref{eq:doubleIndex}, we make the
following observation.
If $\estcondP{\docRV=\doc}{\queryRV=\query_j}$ is a normalized query
likelihood value (see Equation \ref{eq:normQL}), then it is quite low
for documents not highly ranked with respect to $\query_j$ by the
query likelihood method; i.e., documents not in
$\retListParm{\ql}{\query_j}$.
This leads us to the following approximation which was also used to
derive the standard \rmOne.
(Refer back to Section \ref{sec:singleQuery} for details.)

We set $\estcondP{\docRV=\doc}{\queryRV=\query_j}=0$ for documents
$\doc$ not in $\retListParm{\ql}{\query_j}$, and to the normalized query
likelihood of $\doc$ with respect to $\query_j$ for documents in
$\retListParm{\ql}{\query_j}$.
Then, we use Dirichlet smoothed document language models:
$\estcondP{\arbTerm}{\docRV=\doc} \definedas
\dirProb{\arbTerm}{\doc}$.
Finally, we omit random variables to alleviate notation.
The resultant relevance-model estimate is:
%\begin{equation}
%\label{eq:basicFuse}
%\condP{\arbTerm}{\relModel} \definedas \sum_{\doc_i \in \cup_{j} \retListParm{\ql}{\query_j}} \dirProb{\arbTerm}{\doc_i} \frac{1}{\numQuery} \sum_{\query_j \in \querySet: \doc_i \in \retListParm{\ql}{\query_j}}  \estcondP{\doc_i}{\query_j}.
%\end{equation}
\begin{equation}
\label{eq:basicFuse}
\condP{\arbTerm}{\relModel} \definedas \sum_{\doc_i \in \cup_{\query_j} \retListParm{\ql}{\query_j}} \dirProb{\arbTerm}{\doc_i}
\frac{1}{\numQuery} \sum_{\query_j \in \querySet:
\doc_i \in
\retListParm{\ql}{\query_j}} \estcondP{\doc}{\query_j}.
%\frac{1}{\numQuery} \sum_{\query_j \in \querySet}  \estcondP{\doc_i}{\query_j}.
\end{equation}


Equation \ref{eq:basicFuse} is essentially \rmOne constructed from
documents in $\cup_{j}\retListParm{\ql}{\query_j}$ which are highly
ranked with respect to at least one query in $\querySet$.
While the weight of a document in the standard \rmOne is its
normalized query likelihood with respect to a single query, here the
weight is $\frac{1}{\numQuery} \sum_{\query_j \in \querySet:
\doc_i \in
\retListParm{\ql}{\query_j}} \estcondP{\doc}{\query_j}$: the average
of $\doc_i$'s normalized query likelihood retrieval scores in the
lists $\retListParm{\ql}{\query_j}$ in which it appears.
These mixture weights are presumably more effective than those
in the single-query case as they are induced using multiple queries.
The document weight, $\frac{1}{\numQuery} \sum_{\query_j \in
\querySet:
\doc_i \in \retListParm{\ql}{\query_j}} \estcondP{\doc_i}{\query_j}$,  is rank equivalent to the score assigned to
$\doc_i$ by fusing the lists $\retListParm{\ql}{\query_j}$ using  \method{CombSUM}~\cite{fox1994combination} (see Eq. \ref{eq:combsum}).
We can fuse the lists
$\retListParm{\ql}{\query_j}$ using other fusion methods.
Then, we can linearly mix, as in \rmOne, the Dirichlet language
models induced from documents in the fused list; the normalized
(fusion) scores of documents in the fused list serve as mixture
weights.
Term-clipping this relevance model and then query anchoring it as was
the case for \rmThree (Section \ref{sec:singleQuery}) we get our
\firstmention{\fuseDocRM} relevance-model estimate.

Finally, we note that there is an important connection between the
relevance-model estimate in Equation~\ref{eq:basicFuse}, which serves
as the foundation of our \method{\fuseDocRM} relevance model, and the \method{\ariRM}
relevance-model estimate from Equation \ref{eq:arith}.
If we use \rmOne rather than \rmThree for $\relModelParm{\query_i}$
in \method{{\ariRM}}, then Equation \ref{eq:arith} becomes:
$$\condP{\arbTerm}{\relModel_{\svar{\ariRM}}} \approx
\estcondP{\arbTerm}{\querySet} \definedas \frac{1}{\numQuery}
\sum_{\query_j \in \querySet} \sum_{\doc_i \in
  \retListParm{\ql}{\query_j}} \dirProb{\arbTerm} {\doc_i}
\estcondP{\doc_i}{\query_j};$$ $\estcondP{\doc_i}{\query_j}$ is
$\doc_i$'s normalized query likelihood with respect to $\query_j$.
Thus, we can arrive to Equation~\ref{eq:basicFuse} by flipping summations.
In other words, linearly fusing {\rmOne}s induced from
the query-likelihood lists, $\retListParm{\ql}{\query_j}$, results in
the same relevance model (\rmOne) as that induced from a list that is the
result of fusing $\set{\retListParm{\ql}{\query_j}}_{j=1}^{\numQuery}$ using \method{CombSUM}.

However, in practice, \method{\ariRM} and \method{\fuseDocRM} can be quite different due
to the fact that \method{\ariRM} fuses clipped relevance models, while
\method{\fuseDocRM} clips a relevance model constructed from the document list
which results from fusing the $\retListParm{\ql}{\query_j}$ lists.
If the highly ranked documents in the fused list, rather than the
entire list, are used to induce \method{\fuseDocRM}, then this further sets it
apart from \method{\ariRM}.\footnote{The query
anchoring applied by \method{\ariRM} and \method{FuseDocRM} is equivalent.}



\subsection{Multiple Relevance Model Retrieval} 
All the methods described thus far induce a single relevance model
that is used to rank the corpus.
We now consider an alternative approach that utilizes multiple
relevance models for multiple retrievals.
Specifically, we fuse the document lists
$\retListParm{\crossEnt}{\relModelParm{\query_1}},\ldots
\retListParm{\crossEnt}{\relModelParm{\query_{\numQuery}}}$; these are
retrieved based on the cross entropy between relevance models (\#3)
induced using the queries in $\querySet$ and document language
models.
This approach, which can use any fusion method, is denoted
\firstmention{\multRM}.

There is a connection between (i) \method{\multRM}, which fuses lists
retrieved in response to multiple relevance models,
(ii) \method{\fuseDocRM} which fuses lists retrieved by the query
likelihood model and then induces a relevance model from the fused
list that is used to rank the corpus, and (iii) \method{\ariRM} which
fuses relevance models and uses the resultant relevance model for
ranking.  Suppose that we induce \rmOne for each query in
$\querySet$ rather than \rmThree~ (i.e., $\relModelParm{\query_i}$
is \rmOne and not \rmThree as was the case heretofore) and we do
not apply term clipping.  Then, the document ranking produced
by \method{\multRM} when using the \method{CombSUM} fusion method
(without retrieval score normalization) is equivalent to that attained
by using the relevance model in Equation~\ref{eq:basicFuse} for
retrieval -- the foundation of
\method{\fuseDocRM} which is equivalent to \method{\ariRM} if an unclipped \rmOne is
used.\footnote{The rank equivalence is due to the linearity of the
cross entropy in its left argument.}
However, in practice, the retrieval models are different due to
applying term clipping and using \rmThree.

